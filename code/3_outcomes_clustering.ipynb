{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering process for outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path\n",
    "input_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering process for outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/3_exit/extract_policies_ML_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean and parse JSON\n",
    "def clean_and_parse_json(json_string):\n",
    "    try:\n",
    "        # Skip invalid strings\n",
    "        if not json_string.strip().startswith(\"{\"):\n",
    "            return None\n",
    "        # Remove trailing commas\n",
    "        cleaned_string = re.sub(r\",\\s*}\", \"}\", json_string)\n",
    "        cleaned_string = re.sub(r\",\\s*]\", \"]\", cleaned_string)\n",
    "        # Parse the cleaned JSON string\n",
    "        return json.loads(cleaned_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "# Function to check if the JSON is meaningful\n",
    "def is_meaningful_json(parsed_data):\n",
    "    if not isinstance(parsed_data, dict):\n",
    "        return False\n",
    "    # Check if the JSON only contains \"None\" values\n",
    "    for key, value in parsed_data.items():\n",
    "        if key != \"None\" or (isinstance(value, dict) and any(k != \"None\" for k in value.keys())):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_items(dataframe):\n",
    "    new_rows = []\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        extracted_data = row['extracted_features_and_correlations']\n",
    "        \n",
    "        # Skip rows with \"No abstract\" or invalid data\n",
    "        if extracted_data == \"No abstract\" or not isinstance(extracted_data, str):\n",
    "            continue\n",
    "        \n",
    "        # Clean and parse JSON content\n",
    "        parsed_data = clean_and_parse_json(extracted_data)\n",
    "        if parsed_data is None or not is_meaningful_json(parsed_data):\n",
    "            print(f\"Skipping non-meaningful JSON for index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        geographic = parsed_data.get(\"GEOGRAPHIC\", \"None\")\n",
    "        policy_index = 0  # Initialize policy index for each row\n",
    "        \n",
    "        # Iterate through the items\n",
    "        for policy, details in parsed_data.items():\n",
    "            if policy == \"GEOGRAPHIC\":  # Skip the geographic key\n",
    "                continue\n",
    "            \n",
    "            # Ensure details is a dictionary before accessing keys\n",
    "            if not isinstance(details, dict):\n",
    "                print(f\"Skipping invalid details for policy {policy} at index {idx}: {details}\")\n",
    "                continue\n",
    "\n",
    "            actor = details.get(\"ACTOR\", \"None\")\n",
    "            mode = details.get(\"MODE\", \"None\")\n",
    "            population = details.get(\"POPULATION\", \"None\")\n",
    "            factors = details.get(\"FACTOR\", {})\n",
    "\n",
    "            policy_index += 1  # Increment policy index\n",
    "            factor_index = 0  # Initialize factor index for each policy\n",
    "\n",
    "            # Iterate through the factors\n",
    "            if isinstance(factors, dict):\n",
    "                for factor, corr_details in factors.items():\n",
    "                    correlation = corr_details.get(\"CORRELATION\", \"None\")\n",
    "                    factor_index += 1  # Increment factor index\n",
    "                    new_rows.append({\n",
    "                        'row_index': idx,  # Use original index as reference\n",
    "                        'policy_index': policy_index,\n",
    "                        'factor_index': factor_index,\n",
    "                        'GEOGRAPHIC': geographic,\n",
    "                        'POLICY': policy,\n",
    "                        'ACTOR': actor,\n",
    "                        'MODE': mode,\n",
    "                        'POPULATION': population,\n",
    "                        'FACTOR': factor,\n",
    "                        'CORRELATION': correlation\n",
    "                    })\n",
    "            else:\n",
    "                # If no factors, add a row for the policy only\n",
    "                factor_index += 1  # Ensure factor index is still incremented\n",
    "                new_rows.append({\n",
    "                    'row_index': idx,\n",
    "                    'policy_index': policy_index,\n",
    "                    'factor_index': factor_index,\n",
    "                    'GEOGRAPHIC': geographic,\n",
    "                    'POLICY': policy,\n",
    "                    'ACTOR': actor,\n",
    "                    'MODE': mode,\n",
    "                    'POPULATION': population,\n",
    "                    'FACTOR': \"None\",\n",
    "                    'CORRELATION': \"None\"\n",
    "                })\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Set multi-level index\n",
    "    new_df.set_index(['row_index', 'policy_index', 'factor_index'], inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extraction\n",
    "extracted_items_df = extract_items(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_columns(df):\n",
    "    # Define a function to concatenate values if they are not \"None\"\n",
    "    def concatenate(row):\n",
    "        values = [row['POLICY'], row['MODE']]\n",
    "        # Filter out \"None\" values and join with a space\n",
    "        return \" \".join(str(value) for value in values if value != \"None\")\n",
    "    \n",
    "    # Apply the function to each row and create a new column\n",
    "    df['concatenated_policy'] = df.apply(concatenate, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the extracted_items_df\n",
    "updated_df = concatenate_columns(extracted_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/4_clustering/2_modif_cluster.csv\",sep=';').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # Import the ast module to parse string representations of lists\n",
    "\n",
    "# Ensure the 'sentences' column in cluster_df is treated as a list\n",
    "cluster_df['sentences'] = cluster_df['sentences'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in updated_df\n",
    "for index, row in updated_df.iterrows():\n",
    "    policy = row['concatenated_policy']\n",
    "    matched_clusters = []  # Collect all matched clusters for the policy\n",
    "\n",
    "    # Check each cluster in cluster_df\n",
    "    for _, cluster_row in cluster_df.iterrows():\n",
    "        # Ensure exact match of the policy in the cluster's sentences list\n",
    "        if policy in cluster_row['sentences']:\n",
    "            matched_clusters.append(cluster_row['Cluster Name'])\n",
    "    \n",
    "    # If matches are found, create duplicate rows\n",
    "    if matched_clusters:\n",
    "        for cluster_index, cluster in enumerate(matched_clusters, start=1):\n",
    "            new_row = row.copy()  # Copy the original row\n",
    "            new_row['matched_cluster'] = cluster\n",
    "            new_row['cluster_index'] = cluster_index  # Assign integer value\n",
    "            results.append(new_row)  # Add the new row to the results\n",
    "    else:\n",
    "        # If no match is found, add the original row with default cluster_index as 1\n",
    "        row['matched_cluster'] = None\n",
    "        row['cluster_index'] = 1  # Default index\n",
    "        results.append(row)\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "expanded_df = pd.DataFrame(results)\n",
    "\n",
    "# Ensure the cluster_index column is an integer type\n",
    "expanded_df['cluster_index'] = expanded_df['cluster_index'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.reset_index().rename(columns={\"level_0\":\"row_index\",\"level_1\":\"policy_index\",\"level_2\":\"factor_index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path  = \"\" \n",
    "  # Update with your desired output path\n",
    "expanded_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/policy_cluster_factor_raw.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows using drop()\n",
    "expanded_df.drop(expanded_df[expanded_df['CORRELATION'] == 'None'].index, inplace = True)\n",
    "expanded_df.drop(expanded_df[expanded_df['CORRELATION'] == 'none'].index, inplace = True)\n",
    "\n",
    "expanded_df.dropna(subset=['matched_cluster'] , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_decrease = [ 'competing', 'regressive', 'worsening']\n",
    "list_increase = ['improving', 'enhancing', 'improving', 'increasing in Germany','increasing in Germany, decreasing in California', 'reproduced or disrupted', 'U-shaped']\n",
    "list_neutral = ['changing', 'competing', 'impact', 'impacted', 'influenced', 'influencing', 'may increase or decrease', 'mixed', 'stable', 'sustaining', 'transformations', 'transforming', 'variability', 'varying']\n",
    "\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_decrease),'CORRELATION'] = 'decreasing'\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_increase),'CORRELATION'] = 'increasing'\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_neutral),'CORRELATION'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path  = \"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/1_policy_cluster_factor_filtered.csv\" \n",
    "  # Update with your desired output path\n",
    "expanded_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_cluster_factor_filtered = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/1_policy_cluster_factor_filtered.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Smaller Model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Ensure Preprocessed Corpus has a Continuous Index\n",
    "preprocessed_corpus = preprocessed_corpus.reset_index(drop=True)\n",
    "\n",
    "# Convert to a list for parallel processing\n",
    "corpus_list = preprocessed_corpus.tolist()\n",
    "\n",
    "# Step 3: Batch Embedding Function\n",
    "def embed_batch(batch):\n",
    "    return embedder.encode(batch, show_progress_bar=False)\n",
    "\n",
    "# Step 4: Generate Embeddings in Batches (Parallelized)\n",
    "def parallel_embedding(corpus, batch_size=512):\n",
    "    embeddings = Parallel(n_jobs=-1)(\n",
    "        delayed(embed_batch)(corpus[i:i + batch_size])\n",
    "        for i in range(0, len(corpus), batch_size)\n",
    "    )\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode the corpus in parallel\n",
    "batch_size = 512\n",
    "corpus_embeddings = parallel_embedding(corpus_list, batch_size=batch_size)\n",
    "\n",
    "# Step 5: Apply Dimensionality Reduction (PCA) Before Normalization\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(corpus_embeddings)\n",
    "reduced_embeddings = normalize(reduced_embeddings)\n",
    "\n",
    "# Step 6: Apply HDBSCAN Clustering\n",
    "# HDBSCAN automatically determines the number of clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,  # Minimum cluster size\n",
    "    min_samples=10,        # Minimum samples in a neighborhood for a core point\n",
    "    metric='euclidean',   # Distance metric\n",
    "    cluster_selection_epsilon=0.5  # Adjust for fine-grained clustering\n",
    ")\n",
    "cluster_assignment = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Step 7: Analyze and Visualize Clusters\n",
    "# HDBSCAN assigns -1 to noise points\n",
    "num_clusters_found = len(set(cluster_assignment)) - (1 if -1 in cluster_assignment else 0)\n",
    "print(f\"Number of clusters found: {num_clusters_found}\")\n",
    "\n",
    "# Group sentences by cluster\n",
    "clustered_sentences = [[] for _ in range(num_clusters_found)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id != -1:  # Exclude noise points\n",
    "        clustered_sentences[cluster_id].append(corpus_list[sentence_id])\n",
    "\n",
    "# Print clusters\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(f\"Cluster {i + 1}:\")\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering re-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Suppress duplicate sentences\n",
    "unique_clustered_sentences = [\n",
    "    list(set(cluster)) for cluster in clustered_sentences\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "data = []\n",
    "for cluster_num, sentences in enumerate(unique_clustered_sentences):\n",
    "    # Convert all sentences to strings\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Get the number of sentences in the cluster\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Randomly sample 10 sentences (or fewer if the cluster has less than 10 sentences)\n",
    "    sample_sentences = random.sample(sentences, min(15, num_sentences))\n",
    "    \n",
    "    # Append the cluster info to the data list\n",
    "    data.append({\n",
    "        \"Cluster Number\": cluster_num + 1,\n",
    "        \"Number of Sentences\": num_sentences,\n",
    "        \"Sample Sentences\": \"; \".join(sample_sentences)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_summary_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path  = \"\" \n",
    "\n",
    "  # Update with your desired output path\n",
    "cluster_summary_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of clusters to subdivide\n",
    "clusters_to_subdivide = [4, 17]\n",
    "\n",
    "\n",
    "# Function to subdivide clusters and create a new dataframe\n",
    "def subdivide_clusters_to_new_dataframe(clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide):\n",
    "    # Create a list for the final combined clusters\n",
    "    combined_clusters = []\n",
    "    new_subclusters = []  # To hold subdivided clusters\n",
    "\n",
    "    # Add clusters that are not being subdivided to the final list\n",
    "    for cluster_id, sentences in enumerate(clustered_sentences):\n",
    "        if (cluster_id + 1) not in clusters_to_subdivide:  # Adjust for 1-based indexing in `clusters_to_subdivide`\n",
    "            combined_clusters.append({\"cluster_id\": cluster_id + 1, \"sentences\": sentences})\n",
    "\n",
    "    # Subdivide the specified clusters\n",
    "    for cluster_index in clusters_to_subdivide:\n",
    "        # Adjust index for 0-based indexing (Python lists)\n",
    "        cluster_id = cluster_index - 1\n",
    "\n",
    "        # Extract the embeddings and sentences for the current cluster\n",
    "        indices = [i for i, cid in enumerate(cluster_assignment) if cid == cluster_id]\n",
    "        if len(indices) < 5:  # HDBSCAN needs at least a few points\n",
    "            continue\n",
    "\n",
    "        cluster_embeddings = reduced_embeddings[indices]\n",
    "        cluster_sentences = [preprocessed_corpus[i] for i in indices]\n",
    "\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=5,  # Minimum cluster size\n",
    "            min_samples=5,        # Minimum samples in a neighborhood for a core point\n",
    "            metric='euclidean',   # Distance metric\n",
    "            cluster_selection_epsilon=0.45  # Adjust for fine-grained clustering\n",
    "            )\n",
    "        hdbscan_labels = hdbscan_model.fit_predict(cluster_embeddings)\n",
    "\n",
    "        # Map each HDBSCAN cluster to the combined list\n",
    "        for hdbscan_cluster_id in set(hdbscan_labels):\n",
    "            if hdbscan_cluster_id == -1:  # Skip noise\n",
    "                continue\n",
    "            new_subclusters.append(\n",
    "                {\n",
    "                    \"cluster_id\": f\"{cluster_index}-{hdbscan_cluster_id}\",\n",
    "                    \"sentences\": [cluster_sentences[i] for i, label in enumerate(hdbscan_labels) if label == hdbscan_cluster_id],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Append subdivided clusters to the remaining clusters\n",
    "    combined_clusters.extend(new_subclusters)\n",
    "\n",
    "    # Convert the combined clusters into a dataframe\n",
    "    new_cluster_df = pd.DataFrame(combined_clusters)\n",
    "    return new_cluster_df\n",
    "\n",
    "# Subdivide selected clusters and create a new dataframe\n",
    "new_cluster_df = subdivide_clusters_to_new_dataframe(\n",
    "    clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide\n",
    ")\n",
    "\n",
    "# Display the new dataframe\n",
    "print(new_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path  = \"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/4_clustering/1_outcomes/extract_clusters_subdivide.csv\" \n",
    "\n",
    "  # Update with your desired output path\n",
    "new_cluster_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_and_merge_clusters(cluster_df, clusters_to_suppress, clusters_to_merge):\n",
    "    \"\"\"\n",
    "    Suppress and merge clusters in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information.\n",
    "    - clusters_to_suppress: List of cluster IDs to suppress.\n",
    "    - clusters_to_merge: Dictionary where keys are clusters to keep, and values are lists of clusters to merge into them.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with suppressed and merged clusters.\n",
    "    \"\"\"\n",
    "    # Suppress clusters\n",
    "    suppressed_df = cluster_df[~cluster_df[\"cluster_id\"].isin(clusters_to_suppress)]\n",
    "\n",
    "    # Merge clusters\n",
    "    for target_cluster, clusters_to_merge_into in clusters_to_merge.items():\n",
    "        # Find the sentences for all clusters to merge\n",
    "        sentences_to_merge = []\n",
    "        for merge_cluster in clusters_to_merge_into:\n",
    "            merge_rows = suppressed_df[suppressed_df[\"cluster_id\"] == merge_cluster]\n",
    "            if not merge_rows.empty:\n",
    "                sentences_to_merge.extend(merge_rows.iloc[0][\"sentences\"])\n",
    "        \n",
    "        # Append sentences to the target cluster\n",
    "        target_row = suppressed_df[suppressed_df[\"cluster_id\"] == target_cluster]\n",
    "        if not target_row.empty:\n",
    "            target_row_index = target_row.index[0]\n",
    "            suppressed_df.at[target_row_index, \"sentences\"] = (\n",
    "                suppressed_df.at[target_row_index, \"sentences\"] + sentences_to_merge\n",
    "            )\n",
    "        \n",
    "        # Remove merged clusters\n",
    "        suppressed_df = suppressed_df[~suppressed_df[\"cluster_id\"].isin(clusters_to_merge_into)]\n",
    "    \n",
    "    # Reset the index for a clean DataFrame\n",
    "    suppressed_df.reset_index(drop=True, inplace=True)\n",
    "    return suppressed_df\n",
    "\n",
    "\n",
    "# Define clusters to suppress\n",
    "clusters_to_suppress = [\n",
    "    1, 8,\"17-1\", \"29-3\", \"86-2\"\n",
    "]\n",
    "\n",
    "# Define clusters to merge\n",
    "clusters_to_merge = {\n",
    "    18: [56],\n",
    "    82: [\"4-2\",\"4-4\",\"54-6\",\"61-2\",\"89-0\",\"44-1\"]\n",
    "}\n",
    "\n",
    "# Apply suppression and merging\n",
    "cleaned_cluster_df = suppress_and_merge_clusters(new_cluster_df, clusters_to_suppress, clusters_to_merge)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_sentences(cluster_df):\n",
    "    \"\"\"\n",
    "    Removes duplicate sentences within each row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information with a 'sentences' column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with unique sentences in each cluster.\n",
    "    \"\"\"\n",
    "    # Ensure each row's \"sentences\" list contains only unique values\n",
    "    cluster_df[\"sentences\"] = cluster_df[\"sentences\"].apply(lambda x: list(set(x)))\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Apply the function to remove duplicates\n",
    "cleaned_cluster_df = remove_duplicate_sentences(cleaned_cluster_df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster_names(cluster_df, cluster_name_mapping):\n",
    "    \"\"\"\n",
    "    Assign cluster names based on the provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing clusters.\n",
    "    - cluster_name_mapping: Dictionary mapping cluster_id to cluster names.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with a new column for cluster names.\n",
    "    \"\"\"\n",
    "    cluster_df = cluster_df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Assign names using the mapping\n",
    "    cluster_df[\"Cluster Name\"] = cluster_df[\"cluster_id\"].map(cluster_name_mapping)\n",
    "\n",
    "    # Fill missing names with \"Unnamed Cluster\"\n",
    "    cluster_df[\"Cluster Name\"].fillna(\"Unnamed Cluster\", inplace=True)\n",
    "\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Define the cluster name mapping (shortened for brevity; use the full mapping provided)\n",
    "cluster_name_mapping = {\n",
    "    18: \"Access, Level and Quality of Service\",\n",
    "    78: \"Accessibility\"\n",
    "}\n",
    "\n",
    "# Apply the function to assign names\n",
    "named_cluster_df = assign_cluster_names(cleaned_cluster_df, cluster_name_mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(named_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path  = \"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/4_clustering/1_outcomes/1_extract_clusters_named.csv\" \n",
    "\n",
    "  # Update with your desired output path\n",
    "named_cluster_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_raw = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/policy_cluster_factor_raw.csv\"  )\n",
    "updated_df = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/3_policy_and_factors_clustered.csv\"  )\n",
    "cluster_df = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/4_clustering/2_modif_cluster.csv\", sep=\";\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df=updated_df.dropna(subset='matched_cluster')\n",
    "updated_df = pd.merge(updated_df, cluster_df[['Cluster Name','Agg Cluster']], how= 'left', left_on= 'matched_cluster', right_on= 'Cluster Name')\n",
    "grouped = updated_df.groupby([\"Agg Cluster\",\"Cluster Name\"])[\"matched_cluster\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "output_path = \"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/6_visuals/radial_tree.csv\" \n",
    "\n",
    "# Update with your desired output path\n",
    "grouped.reset_index().to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
